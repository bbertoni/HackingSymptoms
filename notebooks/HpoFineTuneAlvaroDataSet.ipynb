{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onto\n",
    "import torch\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, dataloader,max_iter=1000):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tokens, labels in dataloader:\n",
    "\n",
    "            for key,_ in tokens.items():\n",
    "                tokens[key] =tokens[key].to('cuda').squeeze()\n",
    "            labels=labels.to('cuda')\n",
    "            \n",
    "            logits = net(tokens)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "            if count>max_iter:\n",
    "                break\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in Alvaro's Dataset\n",
    "\n",
    "pos_samples = pd.read_csv('HPO-Terms-Dataset/HPO-Terms-Dataset/terms-poss.csv')\n",
    "neg_samples = pd.read_csv('HPO-Terms-Dataset/HPO-Terms-Dataset/terms-negs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([pos_samples,neg_samples],axis=0,sort=False).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['target'] = dataset['score']>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename({'txt1':'names_left','txt2':'names_right'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat train and test data sets:\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "n_train_samples = int(0.8*len(dataset))\n",
    "train_data = dataset.iloc[0: n_train_samples]\n",
    "valid_data = dataset.iloc[n_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data loader for training/classification \n",
    "\n",
    "class hpoDataset(Dataset):\n",
    "\n",
    "    def __init__(self,dataset,max_length=20):\n",
    "        self.df = dataset.reset_index(drop=True)\n",
    "        self.tokenizer =  AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.maxlen = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence1 = self.df.iloc[index]['names_left']\n",
    "        sentence2 = self.df.iloc[index]['names_right']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens =self.tokenizer.encode_plus(sentence1, sentence2 ,truncation =True,return_tensors='pt',padding='max_length', max_length = self.maxlen) #Tokenize the sentence\n",
    "        \n",
    "        label = self.df.iloc[index]['target']\n",
    "        \n",
    "        return tokens,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gsc+ dataset into the trainlaoder \n",
    "train_set = hpoDataset(train_data)\n",
    "train_loader = DataLoader(train_set, batch_size = 30, num_workers = 0,shuffle=True)\n",
    "\n",
    "# load gsc+ dataset into the trainlaoder \n",
    "valid_set = hpoDataset(valid_data)\n",
    "valid_loader = DataLoader(valid_set, batch_size = 30, num_workers = 0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HpoSimilarity(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = False):\n",
    "        super(HpoSimilarity, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "        self.bert_layer.to('cuda')\n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #Classification layer\n",
    "        self.cls_layer = nn.Linear(768, 1).to('cuda')\n",
    "\n",
    "    def forward(self, tokens):\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(**tokens)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep.to('cuda'))\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HpoSimilarity()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, train_loader, val_loader):\n",
    "    best_acc = 0\n",
    "    for ep in range(1):\n",
    "\n",
    "        for it, (tokens, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "    #             tokens = tokens.to('cuda')\n",
    "            for key,_ in tokens.items():\n",
    "                tokens[key] =tokens[key].to('cuda').squeeze()\n",
    "            labels=labels.to('cuda')\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(tokens)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1).to('cuda'), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % 1000 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
    "            \n",
    "            if (it + 1) % 5000 == 0:\n",
    "                val_acc, val_loss = evaluate(net, criterion, val_loader,1000)\n",
    "                print(\"{} iteration completed! Validation Accuracy : {}, Validation Loss : {}\".format(it, val_acc, val_loss))\n",
    "                if val_acc>best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    torch.save(net.state_dict(), 'd:/tmp/fine_tune_sentence_pair_AM{}.dat'.format(it))\n",
    "                else:\n",
    "                    break;\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 of epoch 1 complete. Loss : 0.17828981578350067 Accuracy : 0.9000000357627869\n",
      "Iteration 2000 of epoch 1 complete. Loss : 0.0674898624420166 Accuracy : 0.9666666984558105\n",
      "Iteration 3000 of epoch 1 complete. Loss : 0.12143027037382126 Accuracy : 0.9666666984558105\n",
      "Iteration 4000 of epoch 1 complete. Loss : 0.25577622652053833 Accuracy : 0.9000000357627869\n",
      "Iteration 5000 of epoch 1 complete. Loss : 0.03989880159497261 Accuracy : 0.9666666984558105\n",
      "4999 iteration completed! Validation Accuracy : 0.9760932922363281, Validation Loss : 0.06357457403272211\n",
      "Iteration 6000 of epoch 1 complete. Loss : 0.25289568305015564 Accuracy : 0.9000000357627869\n",
      "Iteration 7000 of epoch 1 complete. Loss : 0.033161234110593796 Accuracy : 1.0\n",
      "Iteration 8000 of epoch 1 complete. Loss : 0.1032642126083374 Accuracy : 0.9666666984558105\n",
      "Iteration 9000 of epoch 1 complete. Loss : 0.10855235904455185 Accuracy : 0.9666666984558105\n",
      "Iteration 10000 of epoch 1 complete. Loss : 0.0322815477848053 Accuracy : 1.0\n",
      "9999 iteration completed! Validation Accuracy : 0.9835851788520813, Validation Loss : 0.044429273415518764\n",
      "Iteration 11000 of epoch 1 complete. Loss : 0.0007187333540059626 Accuracy : 1.0\n",
      "Iteration 12000 of epoch 1 complete. Loss : 0.007177543360739946 Accuracy : 1.0\n",
      "Iteration 13000 of epoch 1 complete. Loss : 0.0008397337514907122 Accuracy : 1.0\n",
      "Iteration 14000 of epoch 1 complete. Loss : 0.09665865451097488 Accuracy : 0.9666666984558105\n",
      "Iteration 15000 of epoch 1 complete. Loss : 0.04400556534528732 Accuracy : 0.9666666984558105\n",
      "14999 iteration completed! Validation Accuracy : 0.9882465600967407, Validation Loss : 0.03196683543524682\n",
      "Iteration 16000 of epoch 1 complete. Loss : 0.12203957885503769 Accuracy : 0.9333333969116211\n",
      "Iteration 17000 of epoch 1 complete. Loss : 0.0018867842154577374 Accuracy : 1.0\n",
      "Iteration 18000 of epoch 1 complete. Loss : 0.006031925790011883 Accuracy : 1.0\n",
      "Iteration 19000 of epoch 1 complete. Loss : 0.029065024107694626 Accuracy : 0.9666666984558105\n",
      "Iteration 20000 of epoch 1 complete. Loss : 0.0017541942652314901 Accuracy : 1.0\n",
      "19999 iteration completed! Validation Accuracy : 0.9893454909324646, Validation Loss : 0.02950577105521215\n",
      "Iteration 21000 of epoch 1 complete. Loss : 0.00044184192665852606 Accuracy : 1.0\n",
      "Iteration 22000 of epoch 1 complete. Loss : 0.007573715411126614 Accuracy : 1.0\n",
      "Iteration 23000 of epoch 1 complete. Loss : 0.09790519624948502 Accuracy : 0.9666666984558105\n",
      "Iteration 24000 of epoch 1 complete. Loss : 0.007092741318047047 Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, opti, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, val_loss=evaluate(net, criterion, valid_loader,1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.9912647008895874, Validation Loss : 0.024122397497957235\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy : {}, Validation Loss : {}\".format(val_acc, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
